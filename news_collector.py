"""
ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import re

class NewsCollector:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        self.session = requests.Session()

        # ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ ì‚¬ì „ (ì˜¤íƒ ë°©ì§€ë¥¼ ìœ„í•´ êµ¬ì²´í™”)
        # ê°•í•œ ê¸ì • (ê°€ì¤‘ì¹˜ 2)
        self.strong_positive_keywords = [
            'ê¸‰ë“±', 'í­ë“±', 'ì‹ ê³ ê°€', '52ì£¼ ì‹ ê³ ê°€', 'ì—­ëŒ€ ìµœê³ ', 'ì‚¬ìƒ ìµœê³ ',
            'ëŒ€ê·œëª¨ ìˆ˜ì£¼', 'ëŒ€í˜• ê³„ì•½', 'ì‹¤ì  í˜¸ì¡°', 'ì–´ë‹ ì„œí”„ë¼ì´ì¦ˆ',
            'ëª©í‘œê°€ ìƒí–¥', 'íˆ¬ìì˜ê²¬ ìƒí–¥', 'ë§¤ìˆ˜ ì¶”ì²œ', 'ê°•ë ¥ ë§¤ìˆ˜',
            'í‘ì ì „í™˜', 'ì‹¤ì  ê°œì„ ', 'FDA ìŠ¹ì¸', 'í—ˆê°€ íšë“'
        ]

        # ì¼ë°˜ ê¸ì • (ê°€ì¤‘ì¹˜ 1)
        self.positive_keywords = [
            'ìƒìŠ¹ì„¸', 'ê°•ì„¸', 'í˜¸ì¬', 'ìˆ˜ì£¼', 'ê³„ì•½ ì²´ê²°', 'ê³µê¸‰ ê³„ì•½',
            'ì¦ê°€ì„¸', 'ì„±ì¥ì„¸', 'í™•ëŒ€', 'ê°œì„ ', 'ëŒíŒŒ', 'ë ë¦¬',
            'í˜¸ì¡°', 'ìƒí–¥ ì¡°ì •', 'ê¸ì •ì ', 'ë§¤ì¶œ ì¦ê°€', 'ì´ìµ ì¦ê°€'
        ]

        # ê°•í•œ ë¶€ì • (ê°€ì¤‘ì¹˜ 2)
        self.strong_negative_keywords = [
            'ê¸‰ë½', 'í­ë½', 'ì‹ ì €ê°€', '52ì£¼ ì‹ ì €ê°€', 'ì—­ëŒ€ ìµœì €', 'ì‚¬ìƒ ìµœì €',
            'ëŒ€ê·œëª¨ ì†ì‹¤', 'ì ì ì „í™˜', 'ì‹¤ì  ì‡¼í¬', 'ì–´ë‹ ì‡¼í¬',
            'ëª©í‘œê°€ í•˜í–¥', 'íˆ¬ìì˜ê²¬ í•˜í–¥', 'ë§¤ë„ ì¶”ì²œ', 'íŒŒì‚°', 'ìƒì¥íì§€',
            'íšŒê³„ ë¶€ì •', 'íš¡ë ¹', 'ë°°ì„', 'ë¶„ì‹íšŒê³„'
        ]

        # ì¼ë°˜ ë¶€ì • (ê°€ì¤‘ì¹˜ 1)
        self.negative_keywords = [
            'í•˜ë½ì„¸', 'ì•½ì„¸', 'ì•…ì¬', 'ê°ì†Œ', 'ì¶•ì†Œ', 'ì ì',
            'ë¶€ì§„', 'ìš°ë ¤', 'ê²½ê³ ', 'í•˜í–¥ ì¡°ì •', 'ë¶€ì •ì ',
            'ì†ì‹¤', 'êµ¬ì¡°ì¡°ì •', 'ê°ì›', 'ë§¤ì¶œ ê°ì†Œ', 'ì´ìµ ê°ì†Œ'
        ]

        # ì˜¤íƒ ë°©ì§€: ë¶€ì •ë¬¸ ì•ì— ë¶™ëŠ” í‚¤ì›Œë“œ (ë¬´ì‹œ)
        self.negation_patterns = [
            'ì—†ì´', 'ì•„ë‹Œ', 'ëª»í•œ', 'ì•Šê³ ', 'ì•Šì€', 'ì œì™¸', 'ë¶ˆêµ¬'
        ]

    def _parse_news_time(self, time_str):
        """ë‰´ìŠ¤ ì‹œê°„ íŒŒì‹± (ì˜ˆ: '2024.01.28 07:30' ë˜ëŠ” '07:30')"""
        try:
            now = datetime.now()

            # "2024.01.28 07:30" í˜•ì‹
            if '.' in time_str:
                return datetime.strptime(time_str, '%Y.%m.%d %H:%M')
            # "07:30" í˜•ì‹ (ì˜¤ëŠ˜)
            elif ':' in time_str:
                time_parts = time_str.split(':')
                hour = int(time_parts[0])
                minute = int(time_parts[1])
                return now.replace(hour=hour, minute=minute, second=0, microsecond=0)
        except Exception:
            return None

    def _is_relevant_time(self, pub_time_str):
        """
        ì¥ì „ ì¢…ëª© ì„ ì •ì— ìœ íš¨í•œ ë‰´ìŠ¤ ì‹œê°„ì¸ì§€ í™•ì¸
        - ì „ì¼ 15:00 ~ ë‹¹ì¼ 09:00 (ì¥í›„~ì¥ì „ ì „ì²´)
        - ì¥ì¤‘ ë‰´ìŠ¤(09:00~15:00)ëŠ” ì „ì¼ ê²ƒë§Œ í¬í•¨
        """
        news_time = self._parse_news_time(pub_time_str)
        if not news_time:
            return True  # ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨ ì‹œ í¬í•¨

        now = datetime.now()

        # í˜„ì¬ ì‹œê°„ì´ 09:00 ì´ì „ (ì¥ì „)
        if now.hour < 9:
            # ì „ì¼ 15:00 ~ ë‹¹ì¼ í˜„ì¬ ì‹œê°„
            yesterday_15 = (now - timedelta(days=1)).replace(hour=15, minute=0, second=0, microsecond=0)
            return yesterday_15 <= news_time <= now
        else:
            # ì¥ì¤‘/ì¥í›„: ì „ì¼ 15:00 ~ ë‹¹ì¼ 09:00
            yesterday_15 = (now - timedelta(days=1)).replace(hour=15, minute=0, second=0, microsecond=0)
            today_0900 = now.replace(hour=9, minute=0, second=0, microsecond=0)
            return yesterday_15 <= news_time <= today_0900

    def _get_news_timing_category(self, pub_time_str):
        """
        ë‰´ìŠ¤ ì‹œê°„ëŒ€ ë¶„ë¥˜ (ì ìˆ˜í™”ìš©)
        - morning: ë‹¹ì¼ 06:00~08:30 (ì¥ì „ ìµœê³ )
        - evening: ì „ì¼ 18:00~24:00 (ì¥í›„)
        - afternoon: ì „ì¼ 15:00~18:00 (ì¥ì¤‘ í›„ë°˜)
        - other: ê¸°íƒ€
        """
        news_time = self._parse_news_time(pub_time_str)
        if not news_time:
            return 'other'

        now = datetime.now()
        hour = news_time.hour

        # ë‹¹ì¼ ë‰´ìŠ¤ì¸ì§€ í™•ì¸
        is_today = news_time.date() == now.date()

        if is_today and 6 <= hour < 9:
            return 'morning'  # ì¥ì „ (ë‹¹ì¼ 06:00~09:00)
        elif not is_today and 18 <= hour <= 23:
            return 'evening'  # ì¥í›„ (ì „ì¼ 18:00~24:00)
        elif not is_today and 15 <= hour < 18:
            return 'afternoon'  # ì¥ì¤‘ í›„ë°˜ (ì „ì¼ 15:00~18:00)
        else:
            return 'other'

    def _analyze_sentiment(self, text):
        """
        ë‰´ìŠ¤ ê°ì„± ë¶„ì„ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)
        - ê°•í•œ í‚¤ì›Œë“œëŠ” ê°€ì¤‘ì¹˜ 2, ì¼ë°˜ í‚¤ì›Œë“œëŠ” ê°€ì¤‘ì¹˜ 1
        - ë¶€ì •ë¬¸ íŒ¨í„´ ì•ì— ìˆëŠ” í‚¤ì›Œë“œëŠ” ë¬´ì‹œ
        """
        positive_score = 0
        negative_score = 0
        matched_positive = []
        matched_negative = []

        # ê°•í•œ ê¸ì • í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ 2)
        for keyword in self.strong_positive_keywords:
            if keyword in text:
                # ë¶€ì •ë¬¸ íŒ¨í„´ ì²´í¬
                idx = text.find(keyword)
                context = text[max(0, idx-5):idx]
                if not any(neg in context for neg in self.negation_patterns):
                    positive_score += 2
                    matched_positive.append(keyword)

        # ì¼ë°˜ ê¸ì • í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ 1)
        for keyword in self.positive_keywords:
            if keyword in text:
                idx = text.find(keyword)
                context = text[max(0, idx-5):idx]
                if not any(neg in context for neg in self.negation_patterns):
                    positive_score += 1
                    matched_positive.append(keyword)

        # ê°•í•œ ë¶€ì • í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ 2)
        for keyword in self.strong_negative_keywords:
            if keyword in text:
                idx = text.find(keyword)
                context = text[max(0, idx-5):idx]
                if not any(neg in context for neg in self.negation_patterns):
                    negative_score += 2
                    matched_negative.append(keyword)

        # ì¼ë°˜ ë¶€ì • í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ 1)
        for keyword in self.negative_keywords:
            if keyword in text:
                idx = text.find(keyword)
                context = text[max(0, idx-5):idx]
                if not any(neg in context for neg in self.negation_patterns):
                    negative_score += 1
                    matched_negative.append(keyword)

        # ì ìˆ˜ ì°¨ì´ë¡œ íŒë‹¨ (ìµœì†Œ 2ì  ì°¨ì´ í•„ìš”)
        diff = positive_score - negative_score
        if diff >= 2:
            return 'positive', diff
        elif diff <= -2:
            return 'negative', abs(diff)
        else:
            return 'neutral', 0

    def get_stock_news(self):
        """ë„¤ì´ë²„ ê¸ˆìœµ ì£¼ìš” ë‰´ìŠ¤ ìˆ˜ì§‘ (ì‹œê°„ í•„í„°ë§ + ê°ì„± ë¶„ì„)"""
        print("ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")

        all_news = []

        try:
            # ë„¤ì´ë²„ ê¸ˆìœµ ì¦ì‹œ ë‰´ìŠ¤
            url = 'https://finance.naver.com/news/news_list.naver?mode=LSS3D&section_id=101&section_id2=258&section_id3=401'

            response = self.session.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')

            # ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ (2026ë…„ êµ¬ì¡° ë³€ê²½ ëŒ€ì‘)
            news_list = soup.find('ul', {'class': 'realtimeNewsList'})

            if news_list:
                # dd íƒœê·¸ì—ì„œ ë‰´ìŠ¤ ì¶”ì¶œ
                subjects = news_list.find_all('dd', {'class': 'articleSubject'})
                summaries = news_list.find_all('dd', {'class': 'articleSummary'})

                for i, subject in enumerate(subjects):
                    try:
                        title_tag = subject.find('a')
                        if not title_tag:
                            continue

                        title = title_tag.text.strip()
                        link = 'https://finance.naver.com' + title_tag.get('href', '')

                        # ìš”ì•½ ë° ì‹œê°„
                        summary = ''
                        pub_time = ''
                        if i < len(summaries):
                            summary_dd = summaries[i]
                            # ìš”ì•½ í…ìŠ¤íŠ¸ (span ì œì™¸)
                            for text in summary_dd.stripped_strings:
                                if text and not text in ['ì—°í•©ë‰´ìŠ¤TV', 'ë§¤ì¼ê²½ì œ', 'ì„œìš¸ê²½ì œ', 'í•œêµ­ê²½ì œ', 'ì´ë°ì¼ë¦¬', 'íŒŒì´ë‚¸ì…œë‰´ìŠ¤', '|']:
                                    summary = text
                                    break

                            # ì‹œê°„
                            time_tag = summary_dd.find('span', {'class': 'wdate'})
                            pub_time = time_tag.text.strip() if time_tag else ''

                        # ì‹œê°„ í•„í„°ë§ (ì „ì¼ 15:00 ~ ë‹¹ì¼ 09:00)
                        if not self._is_relevant_time(pub_time):
                            continue

                        # ê°ì„± ë¶„ì„
                        full_text = title + ' ' + summary
                        sentiment, score = self._analyze_sentiment(full_text)

                        # ì‹œê°„ëŒ€ ë¶„ë¥˜ (ì ìˆ˜í™”ìš©)
                        timing_category = self._get_news_timing_category(pub_time)

                        all_news.append({
                            'title': title,
                            'summary': summary,
                            'link': link,
                            'pub_time': pub_time,
                            'source': 'naver_stock',
                            'sentiment': sentiment,
                            'sentiment_score': score,
                            'timing_category': timing_category
                        })

                    except Exception as e:
                        continue

            print(f"  âœ“ ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤: {len(all_news)}ê°œ (ì‹œê°„ í•„í„°ë§ ì ìš©)")

        except Exception as e:
            print(f"  âš ï¸  ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        # ì¶”ê°€ ë‰´ìŠ¤ ì†ŒìŠ¤ (ì¦ê¶Œì‚¬ ë¦¬ì„œì¹˜, íŠ¹ì§•ì£¼ ë“±)
        try:
            # íŠ¹ì§•ì£¼ ë‰´ìŠ¤
            url = 'https://finance.naver.com/news/news_list.naver?mode=LSS3D&section_id=101&section_id2=258&section_id3=402'

            response = self.session.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')

            news_list = soup.find('ul', {'class': 'realtimeNewsList'})

            if news_list:
                # dd íƒœê·¸ì—ì„œ ë‰´ìŠ¤ ì¶”ì¶œ
                subjects = news_list.find_all('dd', {'class': 'articleSubject'})
                summaries = news_list.find_all('dd', {'class': 'articleSummary'})

                # ìƒìœ„ 30ê°œ
                for i, subject in enumerate(subjects[:30]):
                    try:
                        title_tag = subject.find('a')
                        if not title_tag:
                            continue

                        title = title_tag.text.strip()
                        link = 'https://finance.naver.com' + title_tag.get('href', '')

                        # ìš”ì•½ ë° ì‹œê°„
                        summary = ''
                        pub_time = ''
                        if i < len(summaries):
                            summary_dd = summaries[i]
                            # ìš”ì•½ í…ìŠ¤íŠ¸ (span ì œì™¸)
                            for text in summary_dd.stripped_strings:
                                if text and not text in ['ì—°í•©ë‰´ìŠ¤TV', 'ë§¤ì¼ê²½ì œ', 'ì„œìš¸ê²½ì œ', 'í•œêµ­ê²½ì œ', 'ì´ë°ì¼ë¦¬', 'íŒŒì´ë‚¸ì…œë‰´ìŠ¤', '|']:
                                    summary = text
                                    break

                            # ì‹œê°„
                            time_tag = summary_dd.find('span', {'class': 'wdate'})
                            pub_time = time_tag.text.strip() if time_tag else ''

                        # ì‹œê°„ í•„í„°ë§ (ì „ì¼ 15:00 ~ ë‹¹ì¼ 09:00)
                        if not self._is_relevant_time(pub_time):
                            continue

                        # ê°ì„± ë¶„ì„
                        full_text = title + ' ' + summary
                        sentiment, score = self._analyze_sentiment(full_text)

                        # ì‹œê°„ëŒ€ ë¶„ë¥˜ (ì ìˆ˜í™”ìš©)
                        timing_category = self._get_news_timing_category(pub_time)

                        all_news.append({
                            'title': title,
                            'summary': summary,
                            'link': link,
                            'pub_time': pub_time,
                            'source': 'naver_featured',
                            'sentiment': sentiment,
                            'sentiment_score': score,
                            'timing_category': timing_category
                        })

                    except Exception as e:
                        continue

            print(f"  âœ“ íŠ¹ì§•ì£¼ ë‰´ìŠ¤: {len(all_news)}ê°œ (ëˆ„ì )")

        except Exception as e:
            print(f"  âš ï¸  íŠ¹ì§•ì£¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        # ê¸ì • ë‰´ìŠ¤ í†µê³„
        positive_count = sum(1 for n in all_news if n.get('sentiment') == 'positive')
        negative_count = sum(1 for n in all_news if n.get('sentiment') == 'negative')
        neutral_count = sum(1 for n in all_news if n.get('sentiment') == 'neutral')

        print(f"  âœ“ ì´ {len(all_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        print(f"    - ê¸ì •: {positive_count}ê°œ | ì¤‘ë¦½: {neutral_count}ê°œ | ë¶€ì •: {negative_count}ê°œ")

        return all_news

    def count_stock_mentions(self, stock_name, news_list):
        """íŠ¹ì • ì¢…ëª©ì´ ë‰´ìŠ¤ì— ì–¸ê¸‰ëœ íšŸìˆ˜ ê³„ì‚°"""
        count = 0

        for news in news_list:
            title = news.get('title', '')
            summary = news.get('summary', '')

            if stock_name in title or stock_name in summary:
                count += 1

        return count

    def extract_keywords_from_news(self, news_list):
        """ë‰´ìŠ¤ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        from collections import Counter
        import config

        all_keywords = []

        for news in news_list:
            title = news.get('title', '')
            summary = news.get('summary', '')
            text = title + ' ' + summary

            # ì„¤ì •ëœ í…Œë§ˆ í‚¤ì›Œë“œ ì°¾ê¸°
            for theme, keywords in config.THEME_KEYWORDS.items():
                for keyword in keywords:
                    if keyword in text:
                        all_keywords.append((theme, keyword))

        # ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ í…Œë§ˆ
        theme_counter = Counter([theme for theme, _ in all_keywords])

        return theme_counter.most_common(10)


if __name__ == '__main__':
    # í…ŒìŠ¤íŠ¸
    collector = NewsCollector()
    news = collector.get_stock_news()

    print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(news)}ê°œ ë‰´ìŠ¤")

    if news:
        print("\nğŸ“° ìµœê·¼ ë‰´ìŠ¤ 5ê°œ:")
        for item in news[:5]:
            print(f"  - {item['title']}")
            print(f"    {item['pub_time']}")

        print("\nğŸ”¥ í•« í…Œë§ˆ:")
        keywords = collector.extract_keywords_from_news(news)
        for theme, count in keywords:
            print(f"  - {theme}: {count}íšŒ ì–¸ê¸‰")
